{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='blue'>Naive Bayes Classifier</font> \n",
    "\n",
    "##### <font color='Green'>Author</font> : <font color='Brown'>Oluwaseun Tope</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='red'>Application Flow</font>\n",
    "\n",
    "Before proceeding with the algorithm, let’s first discuss the lifecycle of any machine learning model. This diagram explains the creation of a Machine Learning model from scratch and then taking the same model further with hyperparameter tuning to increase its accuracy, deciding the deployment strategies for that model and once deployed setting up the logging and monitoring frameworks to generate reports and dashboards based on the client requirements. \n",
    "A typical lifecycle diagram for a machine learning model looks like:\n",
    "\n",
    "<img src=\"MLApplicationFlow.PNG\" width= \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='blue'>Bayes’s Theorem</font> \n",
    "\n",
    "According to the Wikipedia, In probability theory and statistics,**Bayes’s theorem** (alternatively *Bayes’s law* or *Bayes’s rule*) describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "Mathematically, it can be written as:\n",
    "\n",
    "<img src=formula.PNG width=300>\n",
    "\n",
    "Where A and B are events and P(B)≠0\n",
    "* P(A|B) is a conditional probability: the likelihood of event A occurring given that B is true.\n",
    "* P(B|A) is also a conditional probability: the likelihood of event B occurring given that A is true.\n",
    "* P(A) and P(B) are the probabilities of observing A and B respectively; they are known as the marginal probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s understand it with the help of an example:\n",
    "\n",
    "**The problem statement:**\n",
    "\n",
    "There are two machines which manufacture bulbs. Machine 1 produces 30 bulbs per hour and machine 2 produce 20 bulbs per hour. Out of all bulbs produced, 1 % turn out to be defective. Out of all the defective bulbs, the share of each machine is 50%.  What is the probability that a bulb produced by machine 2 is defective?\n",
    "\n",
    "We can write the information given above in mathematical terms as:\n",
    "\n",
    "The probability that a bulb was made by Machine 1, P(M1)=30/50=0.6\n",
    "\n",
    "The probability that a bulb was made by Machine 2, P(M2)=20/50=0.4\n",
    "\n",
    "The probability that a bulb is defective, P(Defective)=1%=0.01\n",
    "\n",
    "The probability that a defective bulb came out of Machine 1, P(M1 | Defective)=50%=0.5\n",
    "\n",
    "The probability that a defective bulb came out of Machine 2, P(M2 | Defective)=50%=0.5\n",
    "\n",
    "Now, we need to calculate the probability of a bulb produced by machine 2 is defective i.e.,\n",
    "P(Defective | M2).\n",
    "Using the Bayes Theorem above, it can be written as:\n",
    "\n",
    "$P(Defective | M2)=\\frac { P(M2 | Defective) * P(Defective)} { P(M2)}$\n",
    "\n",
    "Substituting the values, we get:$P(Defective | M2)=\\frac {0.5*0.01}{0.4}= 0.0125$\n",
    "\n",
    "Task for you is to calculate the probability that a bulb produced by machine 1 is defective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Python Implementation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with importing necessary libraries\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scikitplot as skl\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"diabetes.csv\") # Reading the Data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see there few data for columns Glucose, Insulin, skin thickness, BMI and Blood Pressure which have value as 0. That's not possible. You can do a quick search to see that one cannot have 0 values for these. Let's deal with that. we can either remove such data or simply replace it with their respective mean values. Let's do the latter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing zero values with the mean of the column\n",
    "data['BMI'] = data['BMI'].replace(0,data['BMI'].mean())\n",
    "data['BloodPressure'] = data['BloodPressure'].replace(0,data['BloodPressure'].mean())\n",
    "data['Glucose'] = data['Glucose'].replace(0,data['Glucose'].mean())\n",
    "data['Insulin'] = data['Insulin'].replace(0,data['Insulin'].mean())\n",
    "data['SkinThickness'] = data['SkinThickness'].replace(0,data['SkinThickness'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the Outliers\n",
    "\n",
    "q = data['Pregnancies'].quantile(0.98)\n",
    "# we are removing the top 2% data from the Pregnancies column\n",
    "data_cleaned = data[data['Pregnancies']<q]\n",
    "q = data_cleaned['BMI'].quantile(0.99)\n",
    "# we are removing the top 1% data from the BMI column\n",
    "data_cleaned  = data_cleaned[data_cleaned['BMI']<q]\n",
    "q = data_cleaned['SkinThickness'].quantile(0.99)\n",
    "# we are removing the top 1% data from the SkinThickness column\n",
    "data_cleaned  = data_cleaned[data_cleaned['SkinThickness']<q]\n",
    "q = data_cleaned['Insulin'].quantile(0.95)\n",
    "# we are removing the top 5% data from the Insulin column\n",
    "data_cleaned  = data_cleaned[data_cleaned['Insulin']<q]\n",
    "q = data_cleaned['DiabetesPedigreeFunction'].quantile(0.99)\n",
    "# we are removing the top 1% data from the DiabetesPedigreeFunction column\n",
    "data_cleaned  = data_cleaned[data_cleaned['DiabetesPedigreeFunction']<q]\n",
    "q = data_cleaned['Age'].quantile(0.99)\n",
    "# we are removing the top 1% data from the Age column\n",
    "data_cleaned  = data_cleaned[data_cleaned['Age']<q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in data_cleaned:\n",
    "    if plotnumber<=9 :\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.distplot(data_cleaned[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        #plt.ylabel('Salary',fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['Outcome'])\n",
    "y = data['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to scale our data as well\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_scaled = scalar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how our data looks now after scaling.\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will check for multicollinearity using VIF(Variance Inflation factor)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"vif\"] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]\n",
    "vif[\"Features\"] = X.columns\n",
    "\n",
    "#let's check the values\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the VIF values are less than 5 and are very low. That means no multicollinearity. Now, we can go ahead with fitting our data to the model. Before that, let's split our data in test and training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X_scaled,y, test_size= 0.25, random_state = 355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Writing different model files to file\n",
    "with open( 'modelForPrediction.sav', 'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "    \n",
    "with open('standardScalar.sav', 'wb') as f:\n",
    "    pickle.dump(scalar,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = conf_mat[0][0]\n",
    "false_positive = conf_mat[0][1]\n",
    "false_negative = conf_mat[1][0]\n",
    "true_negative = conf_mat[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down the formula for Accuracy\n",
    "Accuracy = (true_positive + true_negative) / (true_positive +false_positive + false_negative + true_negative)\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precison\n",
    "Precision = true_positive/(true_positive+false_positive)\n",
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "Recall = true_positive/(true_positive+false_negative)\n",
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "F1_Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under Curve\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "So far we have been doing grid search to maximise the accuracy of our model. Here, we’ll follow a different approach. We’ll create two models, one with Logistic regression and other with Naïve Bayes and we’ll compare the AUC. The algorithm having a better AUC shall be considered for production deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Naive Bayes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logistic = accuracy_score(y_test,y_pred_logistic)\n",
    "accuracy_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test,y_pred_logistic)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "fpr_logistic, tpr_logistic, thresholds_logistic = roc_curve(y_test, y_pred_logistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_logistic, tpr_logistic, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics  import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_naive=roc_auc_score(y_test,y_pred)\n",
    "auc_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_logistic=roc_auc_score(y_test,y_pred_logistic)\n",
    "auc_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the AUC for Naïve Bayes is more. So, we’ll take that as our production-ready model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Deployment\n",
    "\n",
    "Once the training is completed, we need to expose the trained model as an API for the user to consume it. For prediction, the saved model is loaded first and then the predictions are done using it. If the web app works fine, the same app is deployed to the cloud platform.\n",
    "The flow for that can be shown as:\n",
    "\n",
    "<img src=\"testing_pipeline.PNG\">\n",
    "\n",
    "We'll deploy this model to the AWS Cloud Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
